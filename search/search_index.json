{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Here is my homework page in neuroscience course.","title":"Home"},{"location":"about.html","text":"This table uses css generated content for the caption","title":"About"},{"location":"readme%20-%20Copy.html","text":"Homework 5 Part A: RW rule Fig 1: Trulli, Puglia, Italy. Part B: Uncertainty and Learning Your browser does not support the video tag. #include<iostream> int main(){ std::cout << \"Hello world!\" << std::endl; return 0; } This table uses css generated content for the caption Code For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Homework 5"},{"location":"readme%20-%20Copy.html#homework-5","text":"","title":"Homework 5"},{"location":"readme%20-%20Copy.html#part-a-rw-rule","text":"Fig 1: Trulli, Puglia, Italy.","title":"Part A: RW rule"},{"location":"readme%20-%20Copy.html#part-b-uncertainty-and-learning","text":"Your browser does not support the video tag. #include<iostream> int main(){ std::cout << \"Hello world!\" << std::endl; return 0; } This table uses css generated content for the caption","title":"Part B: Uncertainty and Learning"},{"location":"readme%20-%20Copy.html#code","text":"For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Code"},{"location":"templates.html","text":"Homework 5 Part A: RW rule Fig 1: Trulli, Puglia, Italy. Part B: Uncertainty and Learning Your browser does not support the video tag. #include<iostream> int main(){ std::cout << \"Hello world!\" << std::endl; return 0; } This table uses css generated content for the caption Code For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Homework 5"},{"location":"templates.html#homework-5","text":"","title":"Homework 5"},{"location":"templates.html#part-a-rw-rule","text":"Fig 1: Trulli, Puglia, Italy.","title":"Part A: RW rule"},{"location":"templates.html#part-b-uncertainty-and-learning","text":"Your browser does not support the video tag. #include<iostream> int main(){ std::cout << \"Hello world!\" << std::endl; return 0; } This table uses css generated content for the caption","title":"Part B: Uncertainty and Learning"},{"location":"templates.html#code","text":"For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Code"},{"location":"report-hw04/readme.html","text":"Homework 4 Part A Part B Your browser does not support the video tag. Code For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Homework 4"},{"location":"report-hw04/readme.html#homework-4","text":"","title":"Homework 4"},{"location":"report-hw04/readme.html#part-a","text":"","title":"Part A"},{"location":"report-hw04/readme.html#part-b","text":"Your browser does not support the video tag.","title":"Part B"},{"location":"report-hw04/readme.html#code","text":"For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Code"},{"location":"report-hw05/readme.html","text":"Homework 5 To see extra-report for explanatory information about this howework, click bellow button. extra-report.pdf Part A: RW rule Fig 1: RW-rule for above paradigms Part B: Uncertainty and Learning Fig 2: Blocking implimented by kalman-filter and also using \u03c3 0 2 = 0.01 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 3: Blocking implimented by kalman-filter and also using \u03c3 0 2 = 0 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 4: Unblocking implimented by kalman-filter and also using \u03c3 0 2 = 0.01 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 5: Unblocking implimented by kalman-filter and also using \u03c3 0 2 = 0 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 6: Backward-blocking implimented by kalman-filter and also using \u03c3 0 2 = 0.01 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 7: Backward-blocking implimented by kalman-filter and also using \u03c3 0 2 = 0 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Code For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Homework 5"},{"location":"report-hw05/readme.html#homework-5","text":"To see extra-report for explanatory information about this howework, click bellow button. extra-report.pdf","title":"Homework 5"},{"location":"report-hw05/readme.html#part-a-rw-rule","text":"Fig 1: RW-rule for above paradigms","title":"Part A: RW rule"},{"location":"report-hw05/readme.html#part-b-uncertainty-and-learning","text":"Fig 2: Blocking implimented by kalman-filter and also using \u03c3 0 2 = 0.01 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 3: Blocking implimented by kalman-filter and also using \u03c3 0 2 = 0 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 4: Unblocking implimented by kalman-filter and also using \u03c3 0 2 = 0.01 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 5: Unblocking implimented by kalman-filter and also using \u03c3 0 2 = 0 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 6: Backward-blocking implimented by kalman-filter and also using \u03c3 0 2 = 0.01 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.) Fig 7: Backward-blocking implimented by kalman-filter and also using \u03c3 0 2 = 0 in \u03a3 defenition [VIDEO] (Your browser does not support the video tag.)","title":"Part B: Uncertainty and Learning"},{"location":"report-hw05/readme.html#code","text":"For this homework, I prepared a Github repository to developing all about this. Go to the project","title":"Code"},{"location":"report-hw06/readme.html","text":"Homework 6 Part A and B: TD(0) For some comparative reasons, I showed results for bellow conditions. For \u03b2=2 and \u03b5=0.1 Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.) For \u03b2=2 and \u03b5=0.05 Fig : number of steps ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.) For \u03b2=15 and \u03b5=0.1 Fig : number of steps ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.) You can find more figures (that are changed their parameters) by clicking bellow button. See more figures Part C : Fig : Learning Delay using number of steps Fig : Learning Delay using optimal-path-differance Fig : Learning Delay using optimal-path-ratio Part D : For \u03b2=2 and \u03b5=0.1 Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.) Part E : Only one target target is [3,3] and hole is [5,7] For \u03b2=2 and \u03b5=0.1 Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.) two targets first target is [3,3] and second target is [9,2] and hole is [5,7] For \u03b2=2 and \u03b5=0.1 Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.)","title":"Homework 6"},{"location":"report-hw06/readme.html#homework-6","text":"","title":"Homework 6"},{"location":"report-hw06/readme.html#part-a-and-b-td0","text":"For some comparative reasons, I showed results for bellow conditions.","title":"Part A and B: TD(0)"},{"location":"report-hw06/readme.html#for-2-and-01","text":"Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.)","title":"For \u0002wzxhzdk:51\u0003=2 and \u0002wzxhzdk:52\u0003=0.1"},{"location":"report-hw06/readme.html#for-2-and-005","text":"Fig : number of steps ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.05 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.)","title":"For \u0002wzxhzdk:53\u0003=2 and \u0002wzxhzdk:54\u0003=0.05"},{"location":"report-hw06/readme.html#for-15-and-01","text":"Fig : number of steps ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=15 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.) You can find more figures (that are changed their parameters) by clicking bellow button. See more figures","title":"For \u0002wzxhzdk:55\u0003=15 and \u0002wzxhzdk:56\u0003=0.1"},{"location":"report-hw06/readme.html#part-c","text":"Fig : Learning Delay using number of steps Fig : Learning Delay using optimal-path-differance Fig : Learning Delay using optimal-path-ratio","title":"Part C :"},{"location":"report-hw06/readme.html#part-d","text":"","title":"Part D :"},{"location":"report-hw06/readme.html#for-2-and-01_1","text":"Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.)","title":"For \u0002wzxhzdk:59\u0003=2 and \u0002wzxhzdk:60\u0003=0.1"},{"location":"report-hw06/readme.html#part-e","text":"","title":"Part E :"},{"location":"report-hw06/readme.html#only-one-target","text":"target is [3,3] and hole is [5,7]","title":"Only one target"},{"location":"report-hw06/readme.html#for-2-and-01_2","text":"Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.)","title":"For \u0002wzxhzdk:61\u0003=2 and \u0002wzxhzdk:62\u0003=0.1"},{"location":"report-hw06/readme.html#two-targets","text":"first target is [3,3] and second target is [9,2] and hole is [5,7]","title":"two targets"},{"location":"report-hw06/readme.html#for-2-and-01_3","text":"Fig : number of steps ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path ratio ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : optimal path differance ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : agent path of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 1, 10, 100, 1000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) Fig : value-map of episodes 9970, 9980, 9990, 10000 ( \u03b2=2 , \u03b5=0.1 , \u03b3=0.95, \u03bb=0.8) [VIDEO] (Your browser does not support the video tag.)","title":"For \u0002wzxhzdk:63\u0003=2 and \u0002wzxhzdk:64\u0003=0.1"},{"location":"report-hw07/readme.html","text":"/ Homework 7 Part A Q1 Fig : Q2 Fig : Q3 Fig : [VIDEO] (Your browser does not support the video tag.) [VIDEO] (Your browser does not support the video tag.) Q4 Fig : Q5 [VIDEO] (Your browser does not support the video tag.) [VIDEO] (Your browser does not support the video tag.) Compare [VIDEO] (Your browser does not support the video tag.) [VIDEO] (Your browser does not support the video tag.) Q6 Fig : Q7 Fig : Q8 Fig : Q9 Fig : Fig :","title":"Homework 7"},{"location":"report-hw07/readme.html#homework-7","text":"","title":"Homework 7"},{"location":"report-hw07/readme.html#part-a","text":"","title":"Part A"},{"location":"report-hw07/readme.html#q1","text":"Fig :","title":"Q1"},{"location":"report-hw07/readme.html#q2","text":"Fig :","title":"Q2"},{"location":"report-hw07/readme.html#q3","text":"Fig : [VIDEO] (Your browser does not support the video tag.) [VIDEO] (Your browser does not support the video tag.)","title":"Q3"},{"location":"report-hw07/readme.html#q4","text":"Fig :","title":"Q4"},{"location":"report-hw07/readme.html#q5","text":"[VIDEO] (Your browser does not support the video tag.) [VIDEO] (Your browser does not support the video tag.)","title":"Q5"},{"location":"report-hw07/readme.html#compare","text":"[VIDEO] (Your browser does not support the video tag.) [VIDEO] (Your browser does not support the video tag.)","title":"Compare"},{"location":"report-hw07/readme.html#q6","text":"Fig :","title":"Q6"},{"location":"report-hw07/readme.html#q7","text":"Fig :","title":"Q7"},{"location":"report-hw07/readme.html#q8","text":"Fig :","title":"Q8"},{"location":"report-hw07/readme.html#q9","text":"Fig : Fig :","title":"Q9"}]}